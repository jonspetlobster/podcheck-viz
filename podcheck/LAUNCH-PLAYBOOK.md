# PodCheck Launch Playbook
### "Every claim. Checked."

---

## 1. Pre-Launch (T-14 to T-1)

### Teaser Campaign

**Week 1 (T-14 to T-7): Mystery Phase**

- **Twitter/X:** Post anonymous claim verdicts without naming the product. Examples:
  - "A top health podcaster claimed cold plunges boost dopamine 250%. The actual study says 200-300% for norepinephrine, not dopamine. Grade: C-" 
  - "A finance podcaster said 'inflation has never exceeded 15% in US history.' It hit 23.7% in 1920. Grade: F"
  - End each with: "Something's coming. ðŸ‘€"
- **Reddit:** Drop these as standalone posts in r/podcasts ("Anyone else notice how often podcast hosts get facts wrong?") â€” seed the problem awareness.

**Week 2 (T-7 to T-1): Reveal Phase**

- Reveal the brand. Change Twitter bio. Post the landing page.
- Tweet: "We fact-checked 500+ claims across 7 of the biggest podcasts. Graded every host A through F. Launching [date]. Get early access â†’ [link]"
- Post a blurred leaderboard screenshot â€” visible enough to see letter grades but not names. "Who got the F? Find out [date]."

### Landing Page

**Headline:** Every claim. Checked.  
**Subhead:** AI-powered accuracy grades for the world's biggest podcasts. See who's telling the truth â€” and who's winging it.  
**Body:**
- "We analyze every factual claim made on top podcasts. Each one is verified against primary sources, peer-reviewed research, and public data."
- "Hosts and guests receive letter grades (A-F) based on accuracy, source quality, and correction rate."
- "No ads. No partisan agenda. Just facts."
- Show 3 sample verdict cards (blurred names)
- Email capture: "Get your access code on launch day"
- Counter: "[X] people waiting"

### Beta Waitlist Strategy

Referral-gated: "Move up the list â€” share your unique link. Top 100 referrers get lifetime free access." Use Waitlist.co or custom Supabase setup.

### Seed List â€” 50 People to Contact Pre-Launch

**Tech/Media Journalists (DM or email, T-7):**
1. Casey Newton (@CaseyNewton) â€” Platformer. Covers platform accountability.
2. Taylor Lorenz â€” covers internet culture, would love the "grading influencers" angle
3. Nilay Patel (@raborakavi) â€” The Verge, podcast host himself
4. Ashley Carman â€” Bloomberg, former podcast beat reporter
5. Nick Quah â€” Vulture/Hot Pod, THE podcast industry journalist
6. Sarah Fischer â€” Axios Media Trends
7. Ben Mullin â€” NYT media reporter
8. Max Tani â€” Semafor, media reporter
9. Julia Alexander â€” Netflix/streaming but covers creator economy
10. Kara Swisher (@karaswisher) â€” will amplify anything that holds tech people accountable

**AI/Tech Influencers (Twitter DM, T-5):**
11. Ethan Mollick (@emollick) â€” AI + education, loves novel AI applications
12. Simon Willison (@simonw) â€” will appreciate the technical approach
13. Swyx (@swyx) â€” AI engineering community
14. Riley Goodside (@goodaborakside) â€” prompt engineering, AI applications
15. Robert Scoble (@Scobleizer) â€” early tech adopter, amplifier
16. Packy McCormick (@packym) â€” Not Boring, covers interesting startups
17. Lenny Rachitsky (@lennysan) â€” huge newsletter, product-focused
18. Dan Shipper (@danshipper) â€” Every, AI tools coverage
19. Ben Thompson (@benthompson) â€” Stratechery, media/tech analysis
20. Turner Novak (@TurnerNovak) â€” VC, huge Twitter presence, meme-friendly

**Podcast Hosts & Adjacent (email, T-7):**
21. Andrew Huberman's team â€” give them a heads-up, especially if grade is good
22. David Friedberg (@friedberg) â€” All-In, science guy, would engage
23. Jason Calacanis (@Jason) â€” All-In, will absolutely react publicly
24. Chamath Palihapitiya (@chaaborakmath) â€” All-In, competitive, will want to know his grade
25. Lex Fridman (@lexfridman) â€” values truth-seeking, likely to engage positively
26. Rhonda Patrick (@foundmyfitness) â€” rigorous, likely high grade, will share
27. Steven Levitt / Freakonomics team â€” data-driven, would appreciate methodology
28. Ben Gilbert & David Rosenthal (@acquired_pod) â€” thorough researchers, likely high grade
29. Peter Attia's team (@PeterAttiaMD) â€” medical rigor, will care about methodology

**Newsletter/Community Operators (DM, T-5):**
30. Brian Morrissey (@bmorrissey) â€” The Rebooting, media industry
31. Jacob Donnelly â€” A Media Operator
32. Matt McGarry (@JMatthewMcGarry) â€” newsletter growth expert, will cover the launch
33. Chenell Basilio (@chenellbasilio) â€” Growth in Reverse, newsletter case studies
34. Josh Spector (@jspector) â€” For The Interested
35. Blake Robbins (@blaborakerobbins) â€” VC, podcast listener
36. Mario Gabriele (@mariogabriele) â€” The Generalist
37. Sam Parr (@theSamParr) â€” The Hustle / My First Million
38. Shaan Puri (@ShaanVP) â€” My First Million, loves viral products

**Misinformation/Media Literacy Space:**
39. RenÃ©e DiResta â€” Stanford Internet Observatory, misinfo research
40. Mike Caulfield (@holden) â€” digital literacy researcher
41. Claire Wardle â€” First Draft / information ecosystem
42. Joan Donovan â€” media manipulation researcher
43. Craig Silverman â€” ProPublica, misinformation reporter
44. Brandy Zadrozny â€” NBC News, misinfo beat

**Power Users / Reddit/HN Amplifiers:**
45. Daniel Gross (@danielgross) â€” AI investor, HN royalty
46. Paul Graham (@paulg) â€” if he tweets it, HN follows
47. Austen Allred (@AustenAllred) â€” contrarian, will engage
48. Garry Tan (@garrytan) â€” YC CEO, podcast listener
49. Sahil Lavingia (@shl) â€” Gumroad, indie product community
50. Pieter Levels (@levelsio) â€” indie maker community, will respect the build

**Outreach Template (Twitter DM):**

> Hey [name] â€” building something I think you'd find interesting. We built an AI that fact-checks podcast episodes and grades hosts/guests A-F on accuracy. Covers All-In, Huberman, Lex, etc. Launching [date]. Would love to give you early access â€” some of the results are genuinely surprising. Want a link?

---

## 2. Launch Day Plan

**Timeline (all times PST):**

| Time | Action |
|------|--------|
| 5:00 AM | Product Hunt goes live (auto-publishes at midnight ET) |
| 5:30 AM | Send waitlist email |
| 6:00 AM | Post HN "Show HN" |
| 6:15 AM | Twitter launch thread |
| 6:30 AM | Reddit posts (stagger by 15 min) |
| 7:00 AM | DM all 50 seed list people: "We're live" with link |
| 9:00 AM | Second Twitter push â€” share first interesting data point |
| 12:00 PM | Reddit engagement â€” reply to every comment |
| 2:00 PM | Twitter: "We've been live 8 hours. Here's what we've learned" thread |
| 5:00 PM | Email #2 to waitlist: "Today's most surprising finding" |
| 8:00 PM | Twitter recap of day 1 stats |

### Hacker News Post

**Title:** `Show HN: PodCheck â€“ AI fact-checker that grades podcast hosts A-F on accuracy`

**Text:**
> Hey HN â€” I built PodCheck because I kept hearing claims on podcasts that sounded wrong but I never had time to verify them.
>
> PodCheck uses AI to extract every factual claim from podcast episodes, verify each one against primary sources, and assign letter grades (A-F) to hosts and guests based on accuracy.
>
> Currently covering 7 podcasts: All-In, Huberman Lab, Lex Fridman, Peter Attia's The Drive, Acquired, Freakonomics, and Found My Fitness.
>
> Some findings so far:
> - We've checked 3,000+ individual claims
> - Average accuracy across all shows: [X]%
> - The highest-graded host might surprise you
> - Some popular "facts" that get repeated across multiple shows are just... wrong
>
> Technical details: [brief architecture â€” what models, how verification works, how you handle ambiguity]
>
> No ads, no paywall for basic grades. Trust is the product.
>
> Would love feedback on methodology â€” this is genuinely hard and I want to get it right.

### Reddit Posts

| Subreddit | Title | Angle |
|-----------|-------|-------|
| r/podcasts | "I built an AI that fact-checks podcasts and grades hosts A-F. Here are the results for 7 major shows." | General interest |
| r/technology | "PodCheck: An AI tool that verifies every factual claim in podcast episodes and grades accuracy" | Tech/AI angle |
| r/dataisbeautiful | "[OC] Accuracy ratings of 3,000+ factual claims across 7 top podcasts" | Data visualization |
| r/HubermanLab | "Every factual claim from Huberman Lab, verified: Here's how Dr. Huberman scores on accuracy" | Community-specific |
| r/allin | "All four All-In hosts graded on factual accuracy â€” the results might start a fight" | Community-specific |
| r/lexfridman | "We fact-checked 50 Lex Fridman episodes. Here's how guests scored on accuracy." | Guest-focused |
| r/Biohackers | "We fact-checked the health claims from 3 major science podcasts. Some popular advice is just wrong." | Health angle |
| r/singularity | "Built an AI fact-checker for podcasts â€” here's what it found about AI claims specifically" | AI claims |

### Twitter/X Launch Thread

> ðŸ§µ 1/ We fact-checked 3,000+ claims from the 7 biggest podcasts in tech, science, and finance.
>
> Then we graded every host and guest A through F.
>
> Some people aced it. Some people... didn't.
>
> Introducing PodCheck â†’ [link]

> 2/ Here's how it works:
>
> AI extracts every factual claim from an episode â†’ each claim is verified against primary sources, peer-reviewed research, and public data â†’ hosts/guests get a letter grade.
>
> No opinions scored. Only verifiable facts.

> 3/ The podcasts we cover at launch:
>
> ðŸŽ™ All-In Podcast
> ðŸ§  Huberman Lab
> ðŸŽ¯ Lex Fridman Podcast
> ðŸ¥ The Drive (Peter Attia)
> ðŸ“ˆ Acquired
> ðŸ“Š Freakonomics
> ðŸ§¬ Found My Fitness
>
> More coming soon.

> 4/ First finding: the most "scientific-sounding" podcast isn't always the most accurate.
>
> Citing a study â‰  citing it correctly.

> 5/ We built a category system:
>
> ðŸ”¬ Science/Health
> ðŸ’° Finance
> ðŸ¤– Tech/AI
> ðŸ“š History
> ðŸ¥— Nutrition
>
> Some hosts crush it in their domain but completely fumble outside it.

> 6/ The biggest surprise? [Acquired/Freakonomics] has the highest average accuracy across all episodes we checked.
>
> Turns out doing months of research per episode actually works.

> 7/ The claim that gets repeated across the most podcasts â€” and is wrong:
>
> "[specific widely-repeated claim]"
>
> 4 out of 7 shows repeated it. The actual data says something different. Here's the breakdown â†’ [link to verdict page]

> 8/ Some hosts grade themselves harshly when they self-correct on later episodes. We track that too.
>
> Self-correction rate is part of the grade. Intellectual honesty matters.

> 9/ "But AI makes mistakes too!"
>
> Yes. Every verdict links to its sources. Every grade shows its work. You can dispute any verdict.
>
> We're not the final word. We're the starting point for verification.

> 10/ No ads. No paywall for basic grades. No partisan agenda.
>
> Trust is the product. The moment we compromise accuracy, we're done.
>
> Check your favorite podcast â†’ podcheck.ai

> 11/ This is V1. We're adding more podcasts weekly based on demand.
>
> Which podcast should we fact-check next? Reply below ðŸ‘‡
>
> And if you're a podcaster â€” your DMs are open. We'll walk you through your results before they go public.

### Product Hunt Launch

**Name:** PodCheck  
**Tagline:** AI fact-checker that grades podcasts A-F on accuracy  
**Description:**
> PodCheck analyzes every factual claim made on top podcasts and grades hosts/guests on accuracy (A-F). Currently covering All-In, Huberman Lab, Lex Fridman, Peter Attia, Acquired, Freakonomics, and Found My Fitness. No ads, no agenda â€” just verified facts with linked sources.

**Maker Comment:**
> Hey PH! I'm [name], and I built PodCheck because I realized I was absorbing "facts" from podcasts without ever checking if they were true. Turns out, some of my favorite hosts are great communicators but... not always great with facts. PodCheck extracts every verifiable claim, checks it against primary sources, and gives transparent letter grades. Every verdict shows its work. I'd love your feedback on the methodology â€” this is a hard problem and I want to get it right. ðŸ™

### Waitlist Email

**Subject:** PodCheck is live. Here's your access.

> You signed up to know when PodCheck launched. That's today.
>
> â†’ [CTA: See the grades]
>
> We've fact-checked 3,000+ claims across 7 major podcasts and graded every host.
>
> One quick ask: if you find a verdict interesting (or wrong!), share it. Every verdict page has a share button. This is how we grow â€” and how we get better.
>
> Thanks for believing in this before it existed.

---

## 3. Week 1 Content Calendar

| Day | Twitter | Reddit | Email |
|-----|---------|--------|-------|
| Mon (Launch) | Launch thread + data highlights | All launch posts | Waitlist "we're live" |
| Tue | "Claim of the day" â€” most viral wrong claim | Reply to all comments, engage | â€” |
| Wed | Head-to-head: All-In hosts compared | r/dataisbeautiful visualization | â€” |
| Thu | "The health claim 3 podcasts got wrong" | r/HubermanLab specific findings | "The Weekly Check #1" |
| Fri | Guest grades spotlight â€” who aced it, who bombed | r/podcasts "what we learned" reflection | â€” |
| Sat | Fun stat: "The most confident wrong claim" | â€” | â€” |
| Sun | "You asked, we're adding: [new podcast]" poll | â€” | â€” |

### Maximum Virality Content

**Best individual claims to highlight:**
- A health/supplement claim from Huberman that's nuanced or contested â€” the Huberman subreddit is VERY active and already debates his claims
- Chamath or Sacks making a confident economic prediction that data contradicts â€” the All-In subreddit will go wild
- A guest on Lex getting a surprisingly low grade despite being a credentialed expert

**The Controversial Reveal:**

The All-In hosts, graded individually, will generate the most buzz. The four hosts have massive, opinionated followings who already argue about who's smartest. Revealing their individual grades â€” especially if there's a clear winner and loser â€” guarantees engagement. Chamath and Sacks fans will fight. Jason will tweet about it. Friedberg will probably appreciate the methodology.

**Runner-up:** Huberman's grade on supplement claims specifically. His subreddit (r/HubermanLab, 700K+) already has a skeptic contingent. A nuanced grade (good on neuroscience, weaker on supplement specifics) would get massive engagement.

---

## 4. Podcast Outreach

### Getting Covered by Podcasts

The meta-narrative is your best pitch: "We fact-check podcasts, and now we're ON a podcast." 

**Target shows:**
- My First Million (Shaan + Sam love viral products)
- Lenny's Podcast (product angle)
- The Vergecast (media/tech)
- Hard Fork (NYT â€” Kevin Roose + Casey Newton)
- Decoder (Nilay Patel)
- Pivot (Kara Swisher + Scott Galloway)
- Darknet Diaries (trust/verification angle)

### Pitch Template

> Subject: We graded [YOUR SHOW] on factual accuracy â€” want to see your score?
>
> Hi [name],
>
> I built PodCheck â€” an AI that fact-checks podcasts and grades hosts on accuracy (A-F). We've analyzed [X] episodes of [their show] and I thought you'd want to see your results before they go public.
>
> A few other hosts have already looked at theirs. Some were pleasantly surprised. Some... had notes.
>
> Happy to walk through the methodology â€” it's a genuinely interesting technical and epistemological challenge. Would also make a great segment if you're interested.
>
> [link to their private results page]

### When a Podcaster Reacts

**Positive reaction (they scored well):** Immediately reply publicly: "Appreciate you sharing! Your episode on [topic] had a 94% accuracy rate â€” one of the highest we've seen." Give them a shareable badge/graphic: "PodCheck Verified: A Grade."

**Negative reaction (they're upset):** Stay measured and transparent. "We hear you â€” every verdict links to its sources and methodology. We'd love to have you review specific verdicts you disagree with. Our dispute process is [link]. Getting this right matters more to us than being right."

**They dispute a verdict:** This is GOLD. Publicly engage, show your work, and update if they're right. "You were right â€” we've updated this verdict. This is exactly how the system improves." This builds more trust than being perfect.

---

## 5. Media/Press

### Press Release â€” Key Angles

**For tech press:** "AI-powered accountability tool grades podcast hosts on factual accuracy"  
**For mainstream:** "New tool reveals how often your favorite podcasters get the facts wrong"  
**For podcast industry:** "The fact-checking layer podcasting has been missing"

### 20 Journalists to Pitch

| Name | Outlet | Angle |
|------|--------|-------|
| Nick Quah | Vulture / Hot Pod | Podcast industry disruption |
| Casey Newton | Platformer | Platform accountability |
| Nilay Patel | The Verge | Tech + media intersection |
| Kevin Roose | NYT | AI applications, misinfo |
| Kara Swisher | Pivot / NYT Opinion | Tech accountability |
| Taylor Lorenz | WashPost / Substack | Creator economy |
| Sarah Fischer | Axios | Media trends |
| Ben Mullin | NYT | Media business |
| Craig Silverman | ProPublica | Misinformation |
| Brandy Zadrozny | NBC News | Misinformation |
| Ashley Carman | Bloomberg | Podcasting/audio |
| Max Tani | Semafor | Media business |
| Julia Angwin | Proof News / The Markup | AI accountability |
| Matt Belloni | Puck | Entertainment/media |
| Peter Kafka | Vox/Recode | Media |
| Lucas Shaw | Bloomberg | Entertainment/media |
| Todd Spangler | Variety | Digital media |
| Sarah Perez | TechCrunch | Consumer apps |
| Kyle Wiggers | TechCrunch | AI tools |
| Vittoria Elliott | Wired | AI/platforms |

### Pitch by Outlet Type

**Tech press (Verge, TechCrunch, Wired):** Lead with the AI methodology. How does it work? What's novel? Accuracy of the AI itself. "We built a verification pipeline that checks claims against [X sources]..."

**Mainstream (NYT, NBC, WashPost):** Lead with findings. "The most popular health podcast gets supplement claims wrong X% of the time." Consumer protection angle.

**Podcast industry (Hot Pod, Podnews):** Lead with implications. What does this mean for the industry? Will hosts change behavior? Is this the beginning of accountability in audio?

---

## 6. Ongoing Growth Engine

### Weekly Content Rituals

| Day | Content |
|-----|---------|
| **Monday** | "Weekly Power Rankings" â€” top-graded episodes from last week |
| **Wednesday** | "Claim of the Week" â€” most interesting verdict, visualized |
| **Friday** | "Friday Fails" â€” worst claims of the week (anonymized at first, named if pattern) |

### Automated Twitter/X Strategy

Build a bot (@PodCheckBot or post from main account) that auto-posts:
- New episode grades as they're processed: "New episode graded: [Podcast] E[XX] â€” [Guest] scored [grade]. [X] claims checked. See verdicts â†’ [link]"
- "This week's most repeated wrong claim across podcasts: [claim]"
- Daily "Did you know?" â€” a surprising verdict

### Newsletter: "The Weekly Check"

**Format:**
1. **The Leaderboard** â€” this week's best and worst grades
2. **Claim Spotlight** â€” deep dive on one interesting verdict
3. **The Correction** â€” any verdicts we updated (builds trust)
4. **New Podcasts Added** â€” what's coming
5. **Community Pick** â€” most disputed verdict of the week

Send weekly, Thursday 8 AM PT.

### SEO Strategy

**High-value pages to build:**
- `/podcast/[name]` â€” overall grade page for each podcast (ranks for "[podcast name] accuracy")
- `/podcast/[name]/[episode]` â€” episode-level grades (ranks for episode titles)
- `/person/[name]` â€” individual accuracy profile (ranks for "[person name] fact check")
- `/claim/[topic]` â€” claim category pages ("cold plunge benefits fact check")
- `/verdict/[specific-claim]` â€” individual verdicts (long-tail)

**Target keywords:**
- "[podcast name] fact check" (all 7 podcasts)
- "is [claim] true" (for popular claims)
- "podcast accuracy" / "podcast misinformation"
- "[guest name] claims" / "[guest name] accuracy"
- "huberman lab supplements evidence" (high search volume)

**Content plays:** Blog posts analyzing trends: "Which Podcast Category Has the Most Misinformation?" â€” linkable, shareable, SEO-rich.

### Community

**Start with r/PodCheck subreddit** â€” lower friction than Discord. Post every verdict, let people debate. Moderate lightly.

**Discord later** (1,000+ active users) â€” channels per podcast, #dispute-a-verdict, #suggest-a-podcast.

---

## 7. Guerrilla Tactics

### Manufacturing the First Viral Moment

**The Leaderboard Drop:** On launch day +2, post a clean, shareable graphic ranking all 7 podcasts by accuracy grade. Simple A-F leaderboard. This WILL get screenshotted and shared because people love rankings and will argue about them. Post to Twitter, Reddit, and email it to every seed list contact.

**The Head-to-Head:** "We fact-checked the same topic across 4 podcasts. Here's who got it right." Pick a topic that multiple shows covered (AI risk, GLP-1 drugs, inflation) and show how different hosts handled the same facts. Visualization-friendly, debate-inducing.

### Podcast Community Infiltration

**Subreddits to engage (as a participant, not spammer):**
- r/HubermanLab (780K) â€” already skeptical, will love verification
- r/lexfridman (300K) â€” values truth-seeking
- r/allin (50K) â€” loves host drama
- r/Biohackers (500K) â€” health claims
- r/podcasts (2.5M) â€” general
- r/ChatGPT, r/artificial (AI angle)

**Facebook Groups:** "Huberman Lab Podcast Discussion" (100K+), "Podcast Recommendations" groups

**Strategy:** Don't just drop links. Engage in existing debates about claims and casually mention "actually, we checked this â€” here's what we found." Become the source people cite.

### Creative Plays

1. **"Grade My Podcast" challenge** â€” Let any podcaster submit their RSS feed for grading. Creates inbound demand and FOMO.

2. **Shareable verdict cards** â€” Instagram/Twitter-optimized cards for individual claims. "CLAIM: [X]. VERDICT: [Y]. GRADE: [Z]." Designed to be screenshotted.

3. **Chrome extension** â€” Shows PodCheck grades on Spotify/Apple Podcasts/YouTube pages. Small effort, huge visibility.

4. **"Before You Listen" widget** â€” Embeddable accuracy score that podcast review sites can add.

5. **Bet on the reaction:** Email Chamath, Sacks, Jason, and Friedberg their individual grades simultaneously. At least one will tweet about it within 24 hours. Probably Jason. This alone could drive launch day.

### Partnership Opportunities

- **Snopes / PolitiFact** â€” methodology credibility partnership
- **Podcast hosting platforms** (Spotify for Podcasters, Riverside) â€” embed grades
- **Newsletter platforms** (Substack, Beehiiv) â€” co-marketing to newsletter creators who also podcast
- **Podcast clip tools** (Opus Clip, Podium) â€” show accuracy grades on clips
- **Wikipedia / Wikidata** â€” use as source, contribute back corrections

---

## Key Metrics to Track

| Metric | Launch Day Target | Week 1 Target |
|--------|------------------|---------------|
| Site visits | 10,000 | 50,000 |
| Signups | 2,000 | 8,000 |
| Verdicts viewed | 20,000 | 100,000 |
| Twitter impressions | 500K | 2M |
| HN front page | Yes | â€” |
| Press mentions | 2 | 10 |
| Podcast host reactions | 1 | 3 |

---

## The One Rule

Every piece of content, every interaction, every verdict must reinforce one thing: **PodCheck is accurate, transparent, and has no agenda.** The moment people think you're biased, partisan, or sloppy, trust evaporates and you're just another hot take machine. Show your work. Admit mistakes. Update verdicts publicly. Trust is the product.
